{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8679776,"sourceType":"datasetVersion","datasetId":5202298}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Embedding,Dense\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.sparse import SparseTensor\nfrom tensorflow.keras.activations import softmax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data  = pd.read_csv('/kaggle/input/fren-to-eng/data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_embd = 384\nblock_size = 202\nnum_heads = 6\nhead_size = n_embd//num_heads\nbatch_size = 2\nvocab_size = 259\nsos = 256\neos = 257\npad = 258\ndropout=0.2\nn_layer=6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\nfor x in data['fr_clean']:\n    text+=x\nchars = list(set(text))\nchars.sort()\nchars=chars +[chr(sos),chr(eos)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ch_int = {ch:i for i,ch in enumerate(chars)}\nint_ch = {i:ch for i,ch in enumerate(chars)}\nint_ch[pad]=''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode(text:str)->list:\n    enc = [ord(c) for c in text]\n    enc.insert(0,sos)\n    enc.append(eos)\n    return enc\ndef decode(enc:list)->str:\n    dec = [int_ch[c] for c in enc]\n    text = ''.join(dec)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getBatch():\n    sample = data.sample(n=batch_size)\n    X = []\n    Y = []\n    target = []\n    max_x=0\n    max_y=0\n    for index,row in sample.iterrows():\n        x = encode(row['fr_clean'])\n        y = encode(row['eng_clean'])\n        tar  = y[1:]\n        X.append(x)\n        Y.append(y)\n        target.append(tar)\n        max_x = max(max_x,len(x))\n        max_y = max(max_y,len(y))\n    for i in range(batch_size):\n        while len(X[i])<max_x:\n            X[i].append(pad)\n        while len(Y[i])<max_y:\n            Y[i].append(pad)\n        while len(target[i])<max_y:\n            target[i].append(pad)\n    X = np.array(X) #B*T\n    Y = np.array(Y) #B*T\n    target = np.array(target) #B*T\n    return X,Y,target\n\ndef positional_encoding(T,n_embd):\n    pos = np.arange(T).reshape(T,1)\n    i = np.arange(n_embd).reshape(1,n_embd)//2\n    angles = pos/np.power(10000,2*i/n_embd)\n    angles[:, 0::2] = np.sin(angles[:, 0::2])\n    angles[:, 1::2] = np.cos(angles[:, 1::2])\n    pos_encoding = angles[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# B = Batchsize\n# T = Number of tokens\n# C = Embedding dimension\n# N = Number of Heads\n# K = Head Size\n\n    \n\nclass EncoderHead():\n    def __init__(self):\n        self.key = Dense(head_size,use_bias=False)\n        self.query = Dense(head_size,use_bias=False)\n        self.value = Dense(head_size,use_bias=False)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n\n    def __call__(self,x,pad_mask,training):\n        B,T,C = x.shape\n        k = self.key(x) #B*T*K\n        q = self.query(x) #B*T*K\n        v = self.value(x) #B*T*K\n        K = k.shape[-1]\n        k_T = tf.transpose(k, perm=[0,2,1]) #B*K*T\n        weight = tf.matmul(q,k_T)/K**0.5 #B*T*T\n        pad_mask = pad_mask[:,tf.newaxis,:] #B*1*T\n        pad_mask = tf.tile(pad_mask,[1,T,1]) #B*T*T \n        weight = tf.where(pad_mask==1,float('-inf'),weight) #B*T*T\n        weight =  softmax(weight) #B*T*T\n        weight = self.drop(weight,training=training)\n        out = tf.matmul(weight,v) #B*T*K\n        return out\n    \nclass EncoderMultiHead():\n    def __init__(self):\n        self.heads = [EncoderHead() for _ in range(num_heads)]\n        self.project = Dense(n_embd)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n    \n    def __call__(self,x,pad_mask,training):\n        out = tf.concat([h(x,pad_mask,training) for h in self.heads],axis=-1) #B*T*(K*N)\n        out = self.drop(self.project(out),training=training) #B*T*C\n        return out\n\nclass FeedForward():\n    def __init__(self):\n        self.net =  tf.keras.Sequential([\n            Dense(4*n_embd, activation='relu'),\n            Dense(n_embd),\n        ])\n        self.drop = tf.keras.layers.Dropout(dropout)\n\n    def __call__(self,x,training):\n        return self.drop(self.net(x),training=training) #B*T*C\n\nclass EncodeBlock():\n    def __init__(self):\n        self.sa = EncoderMultiHead()\n        self.frwd = FeedForward()\n        self.lln1 = tf.keras.layers.LayerNormalization(axis=[-1])\n        self.lln2 = tf.keras.layers.LayerNormalization(axis=[-1])\n    \n    def __call__(self,x,pad_mask,training):\n        x = self.lln1(x+self.sa(x,pad_mask,training)) #B*T*C\n        x = self.lln2(x+self.frwd(x,training)) #B*T*C\n        return x\n        \nclass Encoder():\n    def __init__(self):\n        self.tokenEmbedding = Embedding(vocab_size,n_embd)\n        self.blocks = [EncodeBlock() for _ in range(n_layer)]\n    \n    def __call__(self,idx,pad_mask,training):\n        B,T = idx.shape\n        tokEmbd = self.tokenEmbedding(idx) #B * T * C\n        posEmbd = positional_encoding(T,n_embd) #1*T*C\n        embd = tokEmbd+posEmbd #B*T*C\n        for block in self.blocks:\n            embd = block(embd,pad_mask,training) #B*T*C\n        return embd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# t  = Number of tokens in enocoder input\nclass DecoderHead():\n    def __init__(self):\n        self.key = Dense(head_size,use_bias=False)\n        self.query = Dense(head_size,use_bias=False)\n        self.value = Dense(head_size,use_bias=False)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n\n    def __call__(self,y,encoder_output,encoder_pad_mask,training):\n        B,T,C = y.shape\n        t  = encoder_pad_mask.shape[1]\n        k = self.key(encoder_output) #B*t*K\n        q = self.query(y) #B*T*K\n        v = self.value(encoder_output) #B*t*K\n        K = k.shape[-1]\n        k_T = tf.transpose(k, perm=[0,2,1]) #B*K*t\n        weight = tf.matmul(q,k_T)/K**0.5 #B*T*t\n        encoder_pad_mask = encoder_pad_mask[:,tf.newaxis,:] #B*1*t\n        encoder_pad_mask = tf.tile(encoder_pad_mask,[1,T,1]) #B*T*t \n        weight = tf.where(encoder_pad_mask==1,float('-inf'),weight) #B*T*t\n        weight =  softmax(weight) #B*T*t\n        weight = self.drop(weight,training=training)\n        out = tf.matmul(weight,v) #B*T*K\n        return out\n    \nclass DecoderMultiHead():\n    def __init__(self):\n        self.heads = [DecoderHead() for _ in range(num_heads)]\n        self.project = Dense(n_embd)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n    \n    def __call__(self,y,encoder_output, encoder_pad_mask,training):\n        out = tf.concat([h(y,encoder_output, encoder_pad_mask,training) for h in self.heads],axis=-1) #B*T*(K*N)\n        out = self.drop(self.project(out),training=training) #B*T*C\n        return out\nclass DecoderMaskHead():\n    def __init__(self):\n        self.key = Dense(head_size,use_bias=False)\n        self.query = Dense(head_size,use_bias=False)\n        self.value = Dense(head_size,use_bias=False)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n    def createDecoderMask(self,relevantToken:int,totalToken:int):\n        lower_triangular = tf.linalg.band_part(tf.ones((totalToken,totalToken),dtype=tf.int32), -totalToken, 0) #T*T\n        cols = tf.range(totalToken)\n        mask = tf.logical_or(False, cols >= relevantToken) #T*T\n        masked_tensor = tf.where(mask, 0, lower_triangular)      #T*T\n        return masked_tensor\n    def __call__(self,y,decoder_pad_mask,training):\n        B,T,C = y.shape\n        pad_count = np.sum(decoder_pad_mask,axis=1) #B*1\n        attention_mask = tf.stack([self.createDecoderMask(T-i,T) for i in pad_count]) #B*T*T\n        k = self.key(y) #B*T*K\n        q = self.query(y) #B*T*K\n        v = self.value(y) #B*T*K\n        K = k.shape[-1]\n        k_T = tf.transpose(k, perm=[0,2,1]) #B*K*T\n        weight = tf.matmul(q,k_T)/(K**0.5) #B*T*T\n        weight = tf.where(attention_mask==0,float('-inf'),weight) #B*T*T\n        weight =  softmax(weight) #B*T*T\n        weight = self.drop(weight,training=training)\n        out = tf.matmul(weight,v) #B*T*K\n        return out\n    \nclass DecoderMaskMultiHead():\n    def __init__(self):\n        self.heads = [DecoderMaskHead() for _ in range(num_heads)]\n        self.project = Dense(n_embd)\n        self.drop =  tf.keras.layers.Dropout(dropout)\n    \n    def __call__(self,y,decoder_pad_mask,training):\n        out = tf.concat([h(y,decoder_pad_mask,training) for h in self.heads],axis=-1) #B*T*(K*N)\n        out = self.drop(self.project(out),training=training) #B*T*C\n        return out\n      \nclass DecodeBlock():\n    def __init__(self):\n        self.self = DecoderMaskMultiHead()\n        self.cross = DecoderMultiHead()\n        self.frwd = FeedForward()\n        self.lln1 = tf.keras.layers.LayerNormalization(axis=[-1])\n        self.lln2 = tf.keras.layers.LayerNormalization(axis=[-1])\n        self.lln3 = tf.keras.layers.LayerNormalization(axis=[-1])\n    \n    def __call__(self,y,decoder_pad_mask,encoder_output,encoder_pad_mask,training):\n        y = self.lln1(y+self.self(y,decoder_pad_mask,training)) #B*T*C\n        y = self.lln1(y+self.cross(y,encoder_output, encoder_pad_mask,training)) #B*T*C\n        y = self.lln2(y+self.frwd(y,training)) #B*T*C\n        return y\n    \n\n    \n\nclass Decoder():\n    def __init__(self):\n        self.tokenEmbedding = Embedding(vocab_size,n_embd)\n        self.blocks = [DecodeBlock() for _ in range(n_layer)]\n        \n    def __call__(self,decoder_input,decoder_pad_mask,encoder_output,encoder_pad_mask,training):  \n        B,t,C = encoder_output.shape\n        T = decoder_input.shape[1]\n        tokEmbd = self.tokenEmbedding(decoder_input) #B * T * C\n        posEmbd = positional_encoding(T,n_embd) #1*T*C\n        embd = tokEmbd+posEmbd #B*T*C\n        for block in self.blocks:\n            embd = block(embd,decoder_pad_mask,encoder_output,encoder_pad_mask,training) #B*T*C\n        return embd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#R = relevant tokens for loss\nclass Transformer():\n    def __init__(self):\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.lm_head = Dense(vocab_size)\n    \n    def loss(self,encoder_input,decoder_input,targets,training):\n        encoder_pad_mask  = (encoder_input == pad).astype(int) # B*t\n        decoder_pad_mask  = (decoder_input == pad).astype(int) #B*T\n        encoder_output = self.encoder(encoder_input,encoder_pad_mask,training) #B*t*C\n        decoder_output = self.decoder(decoder_input,decoder_pad_mask,encoder_output,encoder_pad_mask,training) #B*T*C\n        logits = self.lm_head(decoder_output) #B*T*V\n        B,T,V = logits.shape\n        logits = tf.reshape(logits,[B*T,V]) #(B*T)*V\n        loss_mask = np.logical_or(decoder_input == sos, decoder_input == eos).astype(int).reshape(B*T) #(B*T)\n        loss_mask = tf.convert_to_tensor(loss_mask==0) # B*T\n        fitered_logits = tf.boolean_mask(logits,loss_mask) #(B*R)*V\n        targets = targets.reshape(B*T) #(B*T)\n        filtered_targets = tf.boolean_mask(targets,loss_mask)#(B*R)\n        m  = filtered_targets.shape[0]\n        idx = tf.Variable(np.arange(m),dtype=tf.int64)\n        filtered_targets = tf.cast(filtered_targets, dtype=tf.int64)\n        indices=tf.stack([idx,filtered_targets],axis=1)\n        filtered_targets = tf.sparse.to_dense(SparseTensor(indices=indices, values=[1]*m, dense_shape=[m,V])) #(B*R)*(vocab_size)\n        loss = CategoricalCrossentropy(from_logits=True)(filtered_targets,fitered_logits)\n        return loss\n    \n    def translate(self,encoder_input):\n        T = encoder_input.shape[1]\n        encoder_pad_mask  = (encoder_input == 258).astype(int) # 1*T\n        encoder_output = self.encoder(encoder_input,encoder_pad_mask,training=False) #1*T*C\n        output = [[256]] \n        i=0\n        while output[0][-1]!=257 and i<2*T:\n            i+=1\n            \n            decoder_input = np.array(output) #1*(current output length)\n            decoder_pad_mask  = (decoder_input == 258).astype(int) #1*(current output length)\n            decoder_output = self.decoder(decoder_input,decoder_pad_mask,encoder_output,encoder_pad_mask,training=False) #1*(current output length)*C\n            logits = self.lm_head(decoder_output) #1*(current output length)*V\n            logits = logits[:,-1,:] #1*V\n            idx_next = tf.random.categorical(logits,1)[0][0].numpy()\n            output[0].append(idx_next)\n        \n        return output[0]\n            \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"B,T = x.shape\nq = x[0].reshape(1,T)\nTransformer().translate(q)\nfor _ in range(max_iters):\n    print(_)\n    xb, yb, target = get_Batch('train')\n    with tf.GradientTape() as tape:\n        logits, loss = model.forward(xb,yb)\n    gradients = tape.gradient(loss, model.tokenEmbeddingTable.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.tokenEmbeddingTable.trainable_variables))\n\n\n\nl= model.generate(tf.zeros([1,1]),1000)[0].numpy().tolist()\nprint(decode(l))","metadata":{},"execution_count":null,"outputs":[]}]}